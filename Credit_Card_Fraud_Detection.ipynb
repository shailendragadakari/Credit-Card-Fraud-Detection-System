{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few rows\n",
    "print(data.head())\n",
    "print(data.describe())\n",
    "\n",
    "# Check for missing values and data types\n",
    "print(data.info())\n",
    "\n",
    "# Check for missing values in each column\n",
    "data.isnull().sum().max()\n",
    "\n",
    "print('No Frauds', round(data['Class'].value_counts()[0]/len(data) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(data['Class'].value_counts()[1]/len(data) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual representation of the dataset\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = [\"#0101DF\", \"#DF0101\"]\n",
    "\n",
    "# Countplot with updated parameters\n",
    "sns.countplot(x='Class', data=data, palette=colors, legend=False)\n",
    "plt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)\n",
    "\n",
    "# Subplots for histograms\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 4))\n",
    "\n",
    "amount_val = data['Amount'].values\n",
    "time_val = data['Time'].values\n",
    "\n",
    "# Histogram for transaction amount\n",
    "sns.histplot(amount_val, ax=ax[0], color='r', kde=True)\n",
    "ax[0].set_title('Distribution of Transaction Amount', fontsize=14)\n",
    "ax[0].set_xlim([min(amount_val), max(amount_val)])\n",
    "\n",
    "\n",
    "# Histogram for transaction time\n",
    "sns.histplot(time_val, ax=ax[1], color='b', kde=True)\n",
    "ax[1].set_title('Distribution of Transaction Time', fontsize=14)\n",
    "ax[1].set_xlim([min(time_val), max(time_val)])\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocessing\n",
    "\n",
    "# Standardize 'Amount' and 'Time'\n",
    "amount_scaler = StandardScaler()\n",
    "time_scaler = StandardScaler()\n",
    "data['scaled_Amount'] = amount_scaler.fit_transform(data['Amount'].values.reshape(-1,1))\n",
    "data['scaled_Time'] = time_scaler.fit_transform(data['Time'].values.reshape(-1,1))\n",
    "\n",
    "# Verify the standardization\n",
    "print(data[['Time', 'Amount', 'scaled_Time' , 'scaled_Amount']].head())\n",
    "\n",
    "# Step 2: Feature Engineering - Add 'Hour' and 'Amount_Bucket'\n",
    "data['Hour'] = (data['scaled_Time'] // 3600) % 24\n",
    "\n",
    "# Since we scaled 'Amount', we need to calculate percentiles and accordingly assign labels to 'Amount_Bucket'\n",
    "percentiles = np.percentile(data['scaled_Amount'], [25, 50, 75, 95])\n",
    "data['Amount_Bucket'] = pd.cut(data['scaled_Amount'], bins=[-float('inf')] + list(percentiles) + [float('inf')], labels=False)\n",
    "print(data)\n",
    "\n",
    "# One-hot encode 'Amount_Bucket'\n",
    "data = pd.get_dummies(data, columns=['Amount_Bucket'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Separate features and target\n",
    "X = data.drop(columns=['Class'])\n",
    "y = data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to evaluate\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100, class_weight='balanced'),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "\n",
    "# Step 4: Perform Stratified K-Fold Cross-Validation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Results storage\n",
    "results = defaultdict(lambda: {\"accuracy\": [], \"roc_auc\": []})\n",
    "\n",
    "# Evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "\n",
    "    fold = 1\n",
    "    for train_idx, test_idx in kfold.split(X, y):\n",
    "        print(f\"  Fold {fold}...\")\n",
    "        # Subset the data for the current fold\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Evaluate accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        results[model_name]['accuracy'].append(accuracy)\n",
    "\n",
    "        # Evaluate ROC AUC\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_proba = model.predict_proba(X_test)[:, 1]\n",
    "            roc_auc = roc_auc_score(y_test, y_proba)\n",
    "            results[model_name]['roc_auc'].append(roc_auc)\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    # Average metrics across folds\n",
    "    avg_accuracy = np.mean(results[model_name]['accuracy'])\n",
    "    avg_roc_auc = np.mean(results[model_name]['roc_auc']) if results[model_name]['roc_auc'] else None\n",
    "\n",
    "    print(f\"  Average Accuracy: {avg_accuracy:.4f}\")\n",
    "    if avg_roc_auc:\n",
    "        print(f\"  Average ROC AUC: {avg_roc_auc:.4f}\")\n",
    "\n",
    "# Summarize results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\nFinal Results for {model_name}:\")\n",
    "    print(f\"  Accuracy: {np.mean(metrics['accuracy']):.4f}\")\n",
    "    if metrics['roc_auc']:\n",
    "        print(f\"  ROC AUC: {np.mean(metrics['roc_auc']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Visualize Results\n",
    "# Accuracy Bar Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=list(models.keys()), y=avg_accuracy)\n",
    "plt.title('Average Accuracy Across Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# ROC-AUC Bar Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=list(models.keys()), y=avg_roc_auc)\n",
    "plt.title('Average ROC-AUC Across Models')\n",
    "plt.ylabel('ROC-AUC')\n",
    "plt.show()\n",
    "\n",
    "# Step 7: Display Classification Reports\n",
    "# Classification Report for Random Forest\n",
    "print(\"\\nDetailed Classification Report for Random Forest (Last Fold):\")\n",
    "model = models['Random Forest']\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Classification Report for Logistic Regression\n",
    "print(\"\\nDetailed Classification Report for Logistic Regression (Last Fold):\")\n",
    "log_reg_model = models['Logistic Regression']\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "y_pred_log_reg = log_reg_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_log_reg))\n",
    "\n",
    "# Classification Report for Decision Tree\n",
    "print(\"\\nDetailed Classification Report for Decision Tree (Last Fold):\")\n",
    "dt_model = models['Decision Tree']\n",
    "dt_model.fit(X_train, y_train)\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "\n",
    "# Classification Report for KNN\n",
    "print(\"\\nDetailed Classification Report for KNN (Last Fold):\")\n",
    "knn_model = models['KNN']\n",
    "knn_model.fit(X_train, y_train)\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "# Step 1: Calculate the Correlation Matrix\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "# Step 2: Visualize the Correlation Matrix using a Heatmap\n",
    "plt.figure(figsize=(12, 8))  # Set the size of the plot\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "\n",
    "# Display the plot\n",
    "plt.title(\"Correlation Matrix of Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data to an Excel file\n",
    "data.to_excel('/content/processed_creditcard_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file to your local machine\n",
    "files.download('/content/processed_creditcard_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load your dataset\n",
    "# Replace 'your_dataset.csv' with the actual dataset path\n",
    "data = pd.read_csv('creditcard.csv')\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = data.drop('Class', axis=1)  # Replace 'Class' with your target column name\n",
    "y = data['Class']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Step 1: Train and Save the Model and Scaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled[['Time', 'Amount']] = scaler.fit_transform(X_train[['Time', 'Amount']])\n",
    "\n",
    "# Train the Random Forest model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the model and scaler\n",
    "joblib.dump(model, 'random_forest_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "print(\"Model and scaler saved successfully.\")\n",
    "\n",
    "# Step 2: Define a function to preprocess the user input\n",
    "def preprocess_input(user_input):\n",
    "    # Load the scaler\n",
    "    scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "    # Convert user input into a dataframe\n",
    "    input_data = pd.DataFrame([user_input], columns=X_train.columns)\n",
    "\n",
    "    # Scale the 'Time' and 'Amount' columns\n",
    "    input_data[['Time', 'Amount']] = scaler.transform(input_data[['Time', 'Amount']])\n",
    "\n",
    "    return input_data\n",
    "\n",
    "# Step 3: Define a function to make a prediction\n",
    "def predict_fraud(user_input):\n",
    "    # Load the trained model\n",
    "    model = joblib.load('random_forest_model.pkl')\n",
    "\n",
    "    # Preprocess the user input\n",
    "    preprocessed_input = preprocess_input(user_input)\n",
    "\n",
    "    # Make the prediction using the model\n",
    "    prediction = model.predict(preprocessed_input)\n",
    "\n",
    "    # Return the prediction result\n",
    "    return \"Fraudulent Transaction\" if prediction[0] == 1 else \"Non-Fraudulent Transaction\"\n",
    "\n",
    "# Step 4: Get user input and make a prediction\n",
    "# This user input is a non fraudulant transaction taken for testing\n",
    "user_input = [150000, -2.123, 0.345, -0.654, 0.567, 1.234, -0.987, 0.456, 0.652, -0.718, 1.385, 0.962, -0.616, 0.238, 0.2489, 0.456, 0.654, 0.567, 1.234, -0.956, 0.3155, 0.168, 0.652, -0.718, -0.123, 0.789, 0.4462,0.568, 1.205, 100.0]\n",
    "\n",
    "# Call the prediction function\n",
    "result = predict_fraud(user_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load your dataset\n",
    "# Replace 'your_dataset.csv' with the actual dataset path\n",
    "data = pd.read_csv('creditcard.csv')\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = data.drop('Class', axis=1)  # Replace 'Class' with your target column name\n",
    "y = data['Class']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Step 1: Train and Save the Model and Scaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled[['Time', 'Amount']] = scaler.fit_transform(X_train[['Time', 'Amount']])\n",
    "\n",
    "# Train the Random Forest model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the model and scaler\n",
    "joblib.dump(model, 'random_forest_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "print(\"Model and scaler saved successfully.\")\n",
    "\n",
    "# Step 2: Define a function to preprocess the user input\n",
    "def preprocess_input(user_input):\n",
    "    # Load the scaler\n",
    "    scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "    # Convert user input into a dataframe\n",
    "    input_data = pd.DataFrame([user_input], columns=X_train.columns)\n",
    "\n",
    "    # Scale the 'Time' and 'Amount' columns\n",
    "    input_data[['Time', 'Amount']] = scaler.transform(input_data[['Time', 'Amount']])\n",
    "\n",
    "    return input_data\n",
    "\n",
    "# Step 3: Define a function to make a prediction\n",
    "def predict_fraud(user_input):\n",
    "    # Load the trained model\n",
    "    model = joblib.load('random_forest_model.pkl')\n",
    "\n",
    "    # Preprocess the user input\n",
    "    preprocessed_input = preprocess_input(user_input)\n",
    "\n",
    "    # Make the prediction using the model\n",
    "    prediction = model.predict(preprocessed_input)\n",
    "\n",
    "    # Return the prediction result\n",
    "    return \"Fraudulent Transaction\" if prediction[0] == 1 else \"Non-Fraudulent Transaction\"\n",
    "\n",
    "# Step 4: Get user input and make a prediction\n",
    "# This user input is a fraudulant transaction taken for testing\n",
    "user_input = [406, -2.312226542, 1.951992011, -1.609850732, 3.997905588, -0.522187865, -1.426545319, -2.537387306, 1.391657248, -2.770089277, -2.772272145, 3.202033207, -2.899907388, -0.595221881, -4.289253782, 0.38972412, -1.14074718,\t-2.830055675, -0.016822468,\t0.416955705, 0.126910559, 0.517232371, -0.035049369, -0.465211076, 0.320198199,\t0.044519167, 0.177839798, 0.261145003, -0.143275875, 0]\n",
    "\n",
    "# Call the prediction function\n",
    "result = predict_fraud(user_input)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
